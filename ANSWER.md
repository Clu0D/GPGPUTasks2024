# 1)

y1 зависит только от нескольких x, это может быть легко параллелизовано, поскольку не нужны синхронизации в процессе работы потоков, нет проблем с подгрузкой в память.

y2 имеет рекурсивную зависимость от самого себя. Наивная реализация потребует по сути последовательный подсчёт y2. 

Если попытаться каждый y2 считать в своём потоке, то, после раскрытия рекурсии, задача сводится к суммированию массива. При этом начнутся проблемы с памятью, поскольку каждому потоку нужен будет весь массив x.

Суммирования массива могут делаться не в одном потоке, а быть ускорены, как обсуждалось на лекции. Но здесь это придётся повторить n раз для разных y2. Может быть ускорено, если в каждом потоке подсчитывать не одно y2[n], а некоторый батч y2[n1..n2].

Есть вариант с подсчётом массива частичных сумм, который затем будет использован для получения y2. В таком случае подсчёт не придётся повторять n раз. Эта задача скорее всего асимптотически равна задаче параллельного подсчёта суммы. 

**Первый сигнал явно проще реализовать на GPU.**

# 2)

```cpp
int idx = get_local_id(1) + get_local_size(1) * get_local_id(0);

// get_local_id(1) - константа в warp
// get_local_size(1) = 32
// тогда 
//      idx = CONST + 32 * i;
// остаток от деления на 32 всегда будет равен CONST % 32
//      а значит сам будет константой для всего warp
//      а значит всегда пойдём по одной ветке if
```

**Сode divergence не будет!**

# 3)

## (a)

```cpp
// для каждого warp
for i in 1 .. 32 
    data[i + 32 * CONST] = 1.0f
```

Потоки warp пишут соседние данные.
**Coalesced.**

Каждый warp обращается к 32 float, то есть к 128 байтам, то есть к 1 кеш линии. В рабочей группе размера (32, 32, 1) будет 32 warp.
**Суммарно получаем 32 кеш линии.**

## (b)

```cpp
// для каждого warp
for i in 1 .. 32 
    data[CONST + 32 * i] = 1.0f
```

Каждый поток будет обращаться через 32 float, то есть каждый раз в новую кеш линию. **Не coalesced.**

32 warp по 32 записи в кеш. **Суммарно 1024 записи в кеш на всю рабочую группу.**

## (c)

```cpp
// для каждого warp
for i in 1 .. 32 
    data[1 + i + 32 * CONST] = 1.0f
```

То же что и в 3.1, но теперь из-за смещения будем читать по 2 кеш линии на warp, поскольку не попадаем в выравнивание.
**Coalesced.**

**64 кеш записей на work group.**
